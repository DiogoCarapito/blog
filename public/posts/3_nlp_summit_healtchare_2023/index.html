<!DOCTYPE html>
<html lang="en">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="UTF-8">
    <title>NLP Summit Healthcare 2023</title>
</head>
<body>
    <article>
        <h1>NLP Summit Healthcare 2023</h1>
        <div><p>This week I attended the <strong><a href="https://www.nlpsummit.org/">NLP Summit Healthcare 2023</a></strong>, a free virtual event organized by the <a href="https://www.johnsnowlabs.com/">John Snow Labs</a>.
It was a great event with a lot of interesting talks. I’ll share some of my key takeaways.</p>
<h2 id="1-best-practices-when-developing-nlp-models">1. Best practices when developing NLP models</h2>
<p>Presented in the opening keynote by Dr. David Gondek, Chief Data Scientist at John Snow Labs, he sumarized some best practices that i found interesting as I’m beginning my NLP journey:</p>
<ul>
<li><strong>Test your models!</strong> - why would you expect untested software to work? Test your models with real data and real use cases and assess it’s performance.</li>
<li><strong>Don’t reuse academic models in production</strong> - published research ≠ building reliable systems, testing and fine-tuning are essencial parts of developing a functional tool.</li>
<li><strong>Test beyond accuracy</strong> - Test for Robustness, Bias, Fairness, Toxicity, Efficiency, Safety. John Snow Labs developed <strong><a href="https://nlptest.org/">NLP test</a></strong> to help with model testing beyond regular benchmarks.</li>
</ul>
<h2 id="2-mitigating-bias-in-healthcare-language-models">2. Mitigating bias in healthcare language models</h2>
<p>Gaurav Kaushik from ScienceIO presented a talk about <strong>mitigating bias</strong> in <strong>healthcare language models</strong> and the importance of its evaluation. <strong>Performance benchmarks</strong> lack the statistical power, they aren’t well validated enough and <strong>don’t incentivize the use of biased systems</strong>.</p>
<ul>
<li><strong>Algorithmic bias</strong>
<ul>
<li>Systematic and repeatable errors that yield unfair outcomes which benefit certain groups over others</li>
<li>Healthcare LP systems will influence clinical outcomes and therefore will mitigate or exacerbate outcome disparities</li>
</ul>
</li>
<li><strong>Selection Bias</strong> - data used in training the model does not represent real-world</li>
<li><strong>Label bias</strong> - mismatch between annotations and target, <em>e.g.</em> due to judgement, human error, or label ambiguity</li>
<li><strong>Training bias</strong> - models amplify biases in the training data</li>
<li><strong>Semantic bias</strong> - bias from input representations such as inappropriate word associations</li>
<li><strong>Demographic bias</strong> - improper sensitivity to race or gender, or impaired performance on attributes related to subgroups</li>
<li><strong>Domain Bias</strong> - error bias in medical subdomains, such as disease areas, which can impair generalization</li>
</ul>
<p>Check this article for further information: <a href="https://aclanthology.org/2020.acl-main.442.pdf">Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</a></p>
<h2 id="3-prototypical-networks-for-interpretable-diagnosis-prediction">3. Prototypical Networks for Interpretable Diagnosis Prediction</h2>
<p><strong>Betty van Aken</strong> from <strong>DATEXIS Research Group</strong> presented a language model that <strong>makes predictions</strong> based on parts of the text that are <strong>similar to prototypical patients</strong> providing justifications that doctors understand.
It uses a prototypical network with label-wise attention to <strong>find the most similar patients to the input text</strong> and then uses a transformer to <strong>predict the diagnosis</strong>.
This is a great example of how NLP can be used in healthcare space and opens the door to a lot of interesting applications.</p>
<p>Check their <a href="https://protopatient.demo.datexis.com/">demo</a> and the paper here: <a href="https://aclanthology.org/2022.aacl-main.14.pdf">This Patient Looks Like That Patient: Prototypical Networks for Interpretable Diagnosis Prediction from Clinical Text</a></p>
<h2 id="4-ehr-safe-generating-high-fidelity-and-privacy-preserving-synthetic-electronic-health-records">4. EHR-Safe: Generating High-Fidelity and Privacy-Preserving Synthetic Electronic Health Records</h2>
<p>AI in healthcare has important <strong>privacy concerns</strong>, especially when dealing with <strong>sensitive data like Electronic Health Records</strong> (EHR). Cloud AI Team suggested that one way to overcome this challenge is to generate high-fidelity, privacy-preserving synthetic EHR data.
They proposed a generative modeling framework, EHR-Safe, that <strong>can generate highly realistic synthetic EHR data</strong> that are robust against privacy attacks.</p>
<p>Check their paper here: <a href="https://ai.googleblog.com/2022/12/ehr-safe-generating-high-fidelity-and.html">EHR-Safe: Generating high-fidelity and privacy-preserving synthetic electronic health records</a></p>
<h2 id="5-some-organizations-that-are-doing-interesting-work-in-nlp-in-healthcare">5. Some organizations that are doing interesting work in NLP in healthcare:</h2>
<ul>
<li><strong><a href="https://www.johnsnowlabs.com/">John Snow Labs</a></strong> - organizer of the summit and creator of state-of-the-art NLP in healthcare, like <a href="https://github.com/johnsnowlabs/nlptest">NLP test</a> and <a href="https://sparknlp.org/">SparkNLP</a></li>
<li><strong><a href="https://discoverylab.ai/">Discovery Lab</a></strong> - Lab funded by Elsevier, Vrije Universiteit Amsterdam and the University of Amsterdam that operates at the crossroads of Knowledge Representation, Machine Learning and Natural Language Processing.</li>
<li><strong><a href="https://www.neuralmed.ai/en">NeuralMed</a></strong> - a healthcare AI company that develops AI solutions for the healthcare industry.</li>
<li><strong><a href="https://www.science.io/">Science IO</a></strong> - transforms medical text into enriched data to build solutions that improve patient care.</li>
<li><strong><a href="https://github.com/DATEXIS">DATEXIS</a></strong> - Data Science and Text-based Information Systems (DATEXIS) group at Beuth University of Applied Sciences Berlin</li>
</ul>
<hr>
<p>This is a list of some interesting projects that were presented at the summit:</p>
<ul>
<li><strong><a href="https://sparknlp.org/">Spark NLP</a></strong> - an open source text processing library for Python, Java, and Scala.</li>
<li><strong><a href="https://nlptest.org/">NLP test</a></strong> - a suit of tests to help mitigate bias for NLP models.</li>
<li><strong><a href="https://huggingface.co/stanford-crfm/BioMedLM">BioMedLM</a></strong> - a GPT style language model trained on biomedical abstracts and papers.</li>
<li><strong><a href="https://huggingface.co/microsoft/biogpt">BioGPT</a></strong> - a pre-trained language models on the biomedical domain.</li>
<li><strong><a href="https://protopatient.demo.datexis.com/">ProtoPatient</a></strong> Demo - diagnostic predictions using clinical text and prototypical patients.</li>
<li><strong><a href="https://huggingface.co/datasets/mitclinicalml/clinical-ie">Clinical IE</a></strong> - a Large Language Model for information extraction of Clinical Text</li>
</ul>
<hr>
<p>Finally, I’ll share some of the <strong>articles</strong> that were cited in the talks:</p>
<ul>
<li><a href="https://arxiv.org/abs/2009.10795">Dataset Carto graphy: Mapping and Diagnosing Datasets with Training Dynamics</a></li>
<li><a href="https://compass.onlinelibrary.wiley.com/doi/10.1111/lnc3.12432">Five sources of bias in natural language processing</a></li>
<li><a href="https://aclanthology.org/2020.acl-main.442.pdf">Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</a></li>
<li><a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00425/108201/Quantifying-Social-Biases-in-NLP-A-Generalization">Quantifying Social Biases in NLP: A Generalization and Empirical Comparison of Extrinsic Fairness Metrics</a></li>
<li><a href="https://arxiv.org/pdf/2109.07958.pdf">TruthfulQA: Measuring How Models Mimic Human Falsehoods</a></li>
<li><a href="https://aclanthology.org/2022.aacl-main.14.pdf">This Patient Looks Like That Patient: Prototypical Networks for Interpretable Diagnosis Prediction from Clinical Text</a></li>
<li><a href="https://assesschild.com/physician-global-assessment">Physician Global Assessment (PGA)</a></li>
<li><a href="https://www.researchsquare.com/article/rs-2347130/v1">EHR-Safe: Generating High-Fidelity and Privacy-Preserving Synthetic Electronic Health Records</a></li>
<li><a href="https://platform.openai.com/docs/guides/fine-tuning">Open AI Fine-tuning guide</a></li>
</ul>
<hr>
<p>There were many more interesting sessions I didn’t describe here, visit <a href="https://www.nlpsummit.org/">https://www.nlpsummit.org/</a> if you want further information.
I’ll be there next year for sure!
Thanks for reading!</p>
<p>P.S. Copilot helped writing this blog post.
LLMs are amazing and I’m excited to see what the future holds for NLP!</p>
</div>
    </article>
</body>
</html>